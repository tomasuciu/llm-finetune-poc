resources:
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      name: terraform-fluxcd-values
    data:
      values.yaml: |
        helmRepository:
          interval: 15m
          nfsServer:
            url: ${soperator_helm_repo}
        certManager:
          enabled: true
          interval: 5m
          timeout: 5m
          %{~ if cert_manager_version != "" ~}
          version: ${cert_manager_version}
          %{~ endif ~}
        backup:
          enabled: ${backups_enabled}
          interval: 5m
          timeout: 5m
          %{~ if k8up_version != "" ~}
          version: ${k8up_version}
          %{~ endif ~}
        mariadbOperator:
          enabled: ${accounting_enabled}
          interval: 5m
          timeout: 5m
          %{~ if mariadb_operator_version != "" ~}
          version: ${mariadb_operator_version}
          %{~ endif ~}
          values: null
        observability:
          enabled: ${telemetry_enabled}
          clusterName: ${cluster_name}
          publicEndpointEnabled: ${public_o11y_enabled}
          region: ${region}
          opentelemetry:
            enabled: ${telemetry_enabled}
            logs:
              %{~ if opentelemetry_collector_version != "" ~}
              version: ${opentelemetry_collector_version}
              %{~ endif ~}
              interval: 5m
              timeout: 5m
              values:
                resources:
                  requests:
                    memory: ${resources.logs_collector.memory}
                    cpu: ${resources.logs_collector.cpu}
                  limits:
                    memory: ${resources.logs_collector.memory}
                jailLogs:
                  enabled: true
                  resources:
                    requests:
                      memory: ${resources.jail_logs_collector.memory}
                      cpu: ${resources.jail_logs_collector.cpu}
                    limits:
                      memory: ${resources.jail_logs_collector.memory}

            events:
              %{~ if opentelemetry_collector_version != "" ~}
              version: ${opentelemetry_collector_version}
              %{~ endif ~}
              interval: 5m
              timeout: 5m
              values:
                resources:
                  requests:
                    memory: ${resources.events_collector.memory}
                    cpu: ${resources.events_collector.cpu}
                  limits:
                    memory: ${resources.events_collector.memory}
          prometheusOperator:
            enabled: ${telemetry_enabled}
            interval: 5m
            timeout: 5m
            %{~ if prometheus_crds_version != "" ~}
            version: ${prometheus_crds_version}
            %{~ endif ~}
          vmStack:
            enabled: ${telemetry_enabled}
            crds:
              interval: 5m
              timeout: 5m
              %{~ if vmstack_crds_version != "" ~}
              version: ${vmstack_crds_version}
              %{~ endif ~}
            interval: 5m
            timeout: 5m
            %{~ if vmstack_version != "" ~}
            version: ${vmstack_version}
            %{~ endif ~}
            values:
              vmagent:
                spec:
                  extraArgs:
                    promscrape.maxScrapeSize: "33554432"
                  externalLabels:
                    iam_tenant_id: ${iam_tenant_id}
                    iam_project_id: ${iam_project_id}
                  resources:
                    requests:
                      memory: ${resources.vm_agent.memory}
                      cpu: ${resources.vm_agent.cpu}
                    limits:
                      memory: ${resources.vm_agent.memory}
                  %{~ if public_o11y_enabled ~}
                  remoteWriteSettings:
                    queues: ${vm_agent_queue_count}
                  %{~ endif ~}
              vmsingle:
                spec:
                  extraArgs:
                    dedup.minScrapeInterval: 30s
                    maxLabelsPerTimeseries: "40"
                    search.maxQueryLen: "18765"
                    search.maxUniqueTimeseries: "500000"
                  retentionPeriod: 30d
                  extraEnvs:
                  - name: GOMAXPROCS
                    value: "${resources.vm_single.gomaxprocs}"
                  %{~ if create_pvcs ~}
                  storage:
                    accessModes:
                      - ReadWriteOnce
                    resources:
                      requests:
                        storage: ${resources.vm_single.size}
                  %{~ endif ~}
                  resources:
                    requests:
                      memory: ${resources.vm_single.memory}
                      cpu: ${resources.vm_single.cpu}
                    limits:
                      memory: ${resources.vm_single.memory}
              prometheusNodeExporter:
                enabled: true
                %{~ if slurm_cluster.use_preinstalled_gpu_drivers ~}
                extraArgs:
                  - '--collector.ethtool.device-exclude=ibp.*'
                  - '--no-collector.infiniband'
                %{~ endif ~}
          vmLogs:
            enabled: ${telemetry_enabled}
            interval: 5m
            timeout: 5m
            %{~ if vmlogs_version != "" ~}
            version: ${vmlogs_version}
            %{~ endif ~}
            values:
              persistentVolume:
              %{~ if create_pvcs ~}
                enabled: true
                size: ${resources.vm_logs.size}
                accessMode: ReadWriteOnce
              %{~ else ~}
                enabled: false
              %{~ endif ~}
              resources:
                requests:
                  memory: ${resources.vm_logs.memory}
                  cpu: ${resources.vm_logs.cpu}
                limits:
                  memory: ${resources.vm_logs.memory}
          dcgmExporter:
            enabled: ${dcgm_job_mapping_enabled}
            version: ${operator_version}
            values:
              hpcJobMapDir: ${dcgm_job_map_dir}
              # waits for nvidia toolkit when not running on preinstalled gpu drivers, so opposite of use_preinstalled_gpu_drivers
              validateToolkit: ${slurm_cluster.use_preinstalled_gpu_drivers ? "false" : "true"}
              resources:
                requests:
                  memory: ${resources.dcgm_exporter.memory}Gi
                  cpu: ${resources.dcgm_exporter.cpu}m
                limits:
                  memory: ${resources.dcgm_exporter.memory}Gi

        securityProfilesOperator:
          enabled: ${apparmor_enabled}
          interval: 5m
          timeout: 5m
          %{~ if security_profiles_operator_version != "" ~}
          version: ${security_profiles_operator_version}
          %{~ endif ~}
          values:
            daemon:
              resources:
                limits:
                  memory: 128Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
            resources:
              limits:
                cpu: 500m
                memory: 3Gi
              requests:
                cpu: 500m
                memory: 3Gi
        slurmCluster:
          enabled: true
          interval: 5m
          timeout: 5m
          version: ${operator_version}
          namespace: soperator
          releaseName: soperator
          overrideValues:
            clusterName: ${name}
            clusterType: ${slurm_cluster.nodes.worker.resources.gpus > 0 ? "gpu" : "cpu" }
            useDefaultAppArmorProfile: ${apparmor_enabled}
            maintenance: ${slurm_cluster.maintenance}

            partitionConfiguration:
              configType: ${slurm_cluster.partition_configuration.slurm_config_type}
              %{~ if slurm_cluster.partition_configuration.slurm_config_type == "custom" ~}
              rawConfig:
                %{~ for partition in slurm_cluster.partition_configuration.slurm_raw_config ~}
                - "${partition}"
                %{~ endfor ~}
              %{~ endif ~}

            workerFeatures:
              %{~ for feature in slurm_cluster.slurm_worker_features ~}
              - name: ${feature.name}
                hostlistExpr: "${feature.hostlist_expr}"
                nodesetName: "${feature.nodeset_name}"
              %{~ endfor ~}

            %{~ if slurm_cluster.slurm_health_check_config != null ~}
            healthCheckConfig:
              healthCheckInterval: ${slurm_cluster.slurm_health_check_config.health_check_interval}
              healthCheckProgram: ${slurm_cluster.slurm_health_check_config.health_check_program}
              healthCheckNodeState:
              %{~ for state in slurm_cluster.slurm_health_check_config.health_check_node_state ~}
                - state: ${state.state}
              %{~ endfor ~}
            %{~ endif ~}

            k8sNodeFilters:
              - name: ${slurm_cluster.k8s_node_filters.system.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.system.match}

              - name: ${slurm_cluster.k8s_node_filters.accounting.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.accounting.match}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.accounting}
                    effect: NoSchedule

              - name: ${slurm_cluster.k8s_node_filters.controller.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.controller.match}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.controller}
                    effect: NoSchedule

              - name: ${slurm_cluster.k8s_node_filters.login.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.login.match}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.login}
                    effect: NoSchedule

              - name: ${slurm_cluster.k8s_node_filters.worker.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                              %{~ for worker in slurm_cluster.k8s_node_filters.worker.matches ~}
                                - "${worker}"
                              %{~ endfor ~}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.worker}
                    effect: NoSchedule
                %{~ if slurm_cluster.nodes.worker.resources.gpus > 0 ~}
                  - key: ${slurm_cluster.k8s_node_filters.label.gpu}
                    operator: Exists
                    effect: NoSchedule
                %{~ endif ~}

            volumeSources:
              - name: jail
                persistentVolumeClaim:
                  claimName: jail-pvc
                  readOnly: false

              - name: controller-spool
                persistentVolumeClaim:
                  claimName: controller-spool-pvc
                  readOnly: false

              - name: worker-spool
                emptyDir:
                  sizeLimit: ${slurm_cluster.nodes.worker.resources.ephemeral_storage}Gi

              %{~ if slurm_cluster.nfs.enabled ~}
              - name: nfs
                nfs:
                  path: ${slurm_cluster.nfs.path}
                  readOnly: false
                  server: ${slurm_cluster.nfs.host}
              %{~ endif ~}

              %{~ if slurm_cluster.nfs_in_k8s.enabled ~}
              - name: home-dir
                createPVC: true
                storageClassName: nfs
                size: 42 # capacity is not enforced by neither NFS server nor csi-driver-nfs
                persistentVolumeClaim:
                  claimName: "home-dir"
              %{~ endif ~}

              %{~ if telemetry_enabled ~}
              - name: motd-nebius-o11y
                configMap:
                  name: motd-nebius-o11y
                  defaultMode: 500
              %{~ endif ~}

              %{~ for sub_mount in slurm_cluster.jail_submounts ~}
              - name: jail-submount-${sub_mount.name}
                persistentVolumeClaim:
                  claimName: jail-submount-${sub_mount.name}-pvc
                  readOnly: false
              %{~ endfor ~}

              - name: sys-host
                hostPath:
                  path: /sys
                  type: Directory

              %{~ if slurm_cluster.use_preinstalled_gpu_drivers~}
              - hostPath:
                  path: /
                  type: ""
                name: nvidia-driver-root
              %{~ endif ~}

              %{~ if slurm_cluster.node_local_image_storage.enabled ~}
              - name: image-storage
                configMap:
                  name: image-storage
                  defaultMode: 500
              %{~ endif ~}

              %{~ if dcgm_job_mapping_enabled ~}
              - name: hpc-jobs-dir
                hostPath:
                  path: ${dcgm_job_map_dir}
                  type: DirectoryOrCreate
              %{~ endif ~}

            populateJail:
              k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.system.name}

            slurmConfig:
              # Allocate half of the available memory by default
              defMemPerNode: ${floor(slurm_cluster.nodes.worker.resources.memory * 1024 / 2)}

            periodicChecks:
              ncclBenchmark:
                enabled: false
                k8sNodeFilterName: system

            slurmNodes:
              accounting:
                enabled: ${accounting_enabled}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.accounting.name}
                %{~ if slurm_cluster.nodes.accounting.enabled ~}
                mariadbOperator:
                  enabled: ${slurm_cluster.nodes.accounting.mariadb_operator.enabled}
                  %{~ if slurm_cluster.nodes.accounting.mariadb_operator.enabled  ~}
                  protectedSecret: ${slurm_cluster.nodes.accounting.use_protected_secret}
                  resources:
                    cpu: ${slurm_cluster.nodes.accounting.mariadb_operator.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.accounting.mariadb_operator.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.accounting.mariadb_operator.resources.ephemeral_storage}Gi
                  metrics:
                    enabled: ${slurm_cluster.nodes.accounting.mariadb_operator.metrics_enabled}
                  storage:
                    ephemeral: false
                    volumeClaimTemplate:
                      accessModes:
                        - ReadWriteOnce
                      resources:
                        requests:
                          storage: ${slurm_cluster.nodes.accounting.mariadb_operator.storage_size}Gi
                  %{~ endif ~}
                %{~ if length(slurm_cluster.nodes.accounting.slurmdbd_config) > 0 ~}
                slurmdbdConfig:
                  %{~ for key, value in slurm_cluster.nodes.accounting.slurmdbd_config ~}
                  ${slurm_cluster.key}: %{ if value == "yes" || value == "no" }"${slurm_cluster.value}"%{ else }${slurm_cluster.value}%{ endif }
                  %{~ endfor ~}
                %{~ endif ~}
                %{~ if length(slurm_cluster.nodes.accounting.slurm_config) > 0 ~}
                slurmConfig:
                  %{~ for key, value in slurm_cluster.nodes.accounting.slurm_config ~}
                  ${slurm_cluster.key}: %{ if value == "yes" || value == "no" }"${slurm_cluster.value}"%{ else }${slurm_cluster.value}%{ endif }
                  %{~ endfor ~}
                %{~ endif ~}
                slurmdbd:
                  resources:
                    cpu: ${slurm_cluster.nodes.accounting.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.accounting.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.accounting.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                %{~ endif ~}

              controller:
                size: ${slurm_cluster.nodes.controller.size}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.controller.name}
                slurmctld:
                  resources:
                    cpu: ${slurm_cluster.nodes.controller.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.controller.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.controller.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                volumes:
                  spool:
                    %{~ if slurm_cluster.controller_state_on_filestore ~}
                    volumeSourceName: "controller-spool"
                    volumeClaimTemplateSpec: null
                    %{~ else ~}
                    volumeClaimTemplateSpec:
                      accessModes:
                        - ReadWriteOnce
                      resources:
                        requests:
                          storage: 50Gi
                      storageClassName: "compute-csi-network-ssd-ext4"
                    %{~ endif ~}
              worker:
                size: ${slurm_cluster.nodes.worker.size}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.worker.name}
                cgroupVersion: v2
                enableGDRCopy: true
                slurmNodeExtra: "${slurm_cluster.nodes.worker.slurm_node_extra}"
                sshdConfigMapRefName: "${slurm_cluster.nodes.worker.sshd_config_map_ref_name}"
                supervisordConfigMapRefName: custom-supervisord-config
                slurmd:
                  resources:
                    cpu: ${slurm_cluster.nodes.worker.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.worker.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.worker.resources.ephemeral_storage}Gi
                    gpu: ${slurm_cluster.nodes.worker.resources.gpus}
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                  - name: ensure-jail-submount-${sub_mount.name}-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: '${sub_mount.name}', mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ endfor ~}
                  %{~ for sub_mount in slurm_cluster.node_local_jail_submounts ~}
                  - name: ensure-node-local-jail-submount-${sub_mount.name}-${sub_mount.filesystem_type}
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: '${sub_mount.name}', mountPath: /volume-mount }]
                    command: ["grep", ' /volume-mount ${sub_mount.filesystem_type} ', "/proc/mounts"]
                  %{~ endfor ~}
                  %{~ if slurm_cluster.node_local_image_storage.enabled ~}
                  - name: ensure-node-local-image-storage-${slurm_cluster.node_local_image_storage.spec.filesystem_type}
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: 'image-storage', mountPath: /volume-mount }]
                    command: ["grep", ' /volume-mount ${slurm_cluster.node_local_image_storage.spec.filesystem_type} ', "/proc/mounts"]
                  - name: prepare-node-local-image-storage
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: 'image-storage', mountPath: /volume-mount }]
                    command:
                    - "sh"
                    - "-c"
                    - |
                      mkdir -p \
                      %{~ for dir in ["cache", "data", "runtime"] ~}
                        /volume-mount/enroot/${dir} \
                      %{~ endfor ~}
                        /volume-mount/docker \
                      && chmod 777 -R /volume-mount/enroot /volume-mount/docker
                  %{~ endif ~}
                volumes:
                  spool:
                    volumeClaimTemplateSpec: null
                    volumeSourceName: worker-spool
                  jailSubMounts:
                    %{~ if slurm_cluster.nfs.enabled ~}
                    - mountPath: ${slurm_cluster.nfs.mount_path}
                      name: nfs
                      volumeSourceName: nfs
                    %{~ endif ~}
                    %{~ if slurm_cluster.nfs_in_k8s.enabled ~}
                    - mountPath: /home
                      name: home-dir
                      volumeSourceName: home-dir
                    %{~ endif ~}
                    %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                    - name: ${sub_mount.name}
                      mountPath: ${sub_mount.mount_path}
                      volumeSourceName: jail-submount-${sub_mount.name}
                    %{~ endfor ~}
                    %{~ for sub_mount in slurm_cluster.node_local_jail_submounts ~}
                    - name: ${sub_mount.name}
                      mountPath: ${sub_mount.mount_path}
                      volumeClaimTemplateSpec:
                        accessModes:
                          - ReadWriteOnce
                        storageClassName: ${sub_mount.storage_class_name}
                        resources:
                          requests:
                            storage: ${sub_mount.size_gibibytes}Gi
                    %{~ endfor ~}
                    %{~ if slurm_cluster.node_local_image_storage.enabled ~}
                    - name: image-storage
                      mountPath: /mnt/image-storage
                      volumeClaimTemplateSpec:
                        accessModes:
                          - ReadWriteOnce
                        storageClassName: ${slurm_cluster.node_local_image_storage.spec.storage_class_name}
                        resources:
                          requests:
                            storage: ${slurm_cluster.node_local_image_storage.spec.size_gibibytes}Gi
                    %{~ endif ~}
                  customMounts:
                    - name: sys-host
                      mountPath: /mnt/jail.upper/sys-host
                      readOnly: true
                      volumeSourceName: sys-host
                    %{~ if slurm_cluster.node_local_image_storage.enabled ~}
                    - name: enroot-on-image-storage
                      mountPath: /etc/enroot/enroot.conf.d/custom-dirs.conf
                      subPath: enroot.conf
                      readOnly: true
                      volumeSourceName: image-storage
                    - name: docker-on-image-storage
                      mountPath: /etc/docker/daemon.json
                      subPath: daemon.json
                      readOnly: true
                      volumeSourceName: image-storage
                    %{~ endif ~}
                    %{~ if dcgm_job_mapping_enabled ~}
                    - name: hpc-jobs-dir
                      mountPath: ${dcgm_job_map_dir}
                      volumeSourceName: hpc-jobs-dir
                    %{~ endif ~}
                    %{~ if slurm_cluster.use_preinstalled_gpu_drivers~}
                    - name: nvidia-driver-root
                      mountPath: /run/nvidia/driver
                      volumeSourceName: nvidia-driver-root
                      readOnly: false
                    %{~ endif ~}
                  sharedMemorySize: ${slurm_cluster.nodes.worker.shared_memory}Gi

              login:
                size: ${slurm_cluster.nodes.login.size}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.login.name}
                sshdServiceType: "LoadBalancer"
                %{~ if slurm_cluster.nodes.login.allocation_id != null ~}
                sshdServiceAnnotations:
                  "nebius.com/load-balancer-allocation-id": "${slurm_cluster.nodes.login.allocation_id}"
                  %{~ if !slurm_cluster.nodes.login.public_ip ~}
                  "nebius.com/load-balancer-type": "internal"
                  %{~ endif ~}
                %{~ endif ~}
                %{~ if length(slurm_cluster.nodes.login.root_public_keys) > 0 ~}
                sshdConfigMapRefName: "${slurm_cluster.nodes.login.sshd_config_map_ref_name}"
                sshRootPublicKeys:
                  %{~ for key in slurm_cluster.nodes.login.root_public_keys ~}
                  - ${key}
                  %{~ endfor ~}
                %{~ endif ~}
                sshd:
                  resources:
                    cpu: ${slurm_cluster.nodes.login.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.login.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.login.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ if tailscale_enabled ~}
                  - name: tailscale
                    image: cr.eu-north1.nebius.cloud/soperator/tailscale:latest # v1.88.4
                    imagePullPolicy: Always
                    restartPolicy: Always
                    securityContext:
                      privileged: true
                    env:
                      - name: POD_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      - name: POD_UID
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.uid
                      - name: TS_DEBUG_FIREWALL_MODE
                        value: auto
                      - name: TS_KUBE_SECRET
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      - name: TS_USERSPACE
                        value: "false"
                  %{~ endif ~}
                  %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                  - name: ensure-jail-submount-${sub_mount.name}-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: '${sub_mount.name}', mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ endfor ~}
                volumes:
                  jailSubMounts:
                    %{~ if slurm_cluster.nfs.enabled ~}
                    - mountPath: ${slurm_cluster.nfs.mount_path}
                      name: nfs
                      volumeSourceName: nfs
                    %{~ endif ~}
                    %{~ if slurm_cluster.nfs_in_k8s.enabled ~}
                    - mountPath: /home
                      name: home-dir
                      volumeSourceName: home-dir
                    %{~ endif ~}
                    %{~ if telemetry_enabled ~}
                    - mountPath: /etc/update-motd.d/95-nebius-o11y
                      subPath: 95-nebius-o11y
                      readOnly: true
                      name: motd-nebius-o11y
                      volumeSourceName: motd-nebius-o11y
                    %{~ endif ~}
                    %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                    - name: ${sub_mount.name}
                      mountPath: ${sub_mount.mount_path}
                      volumeSourceName: jail-submount-${sub_mount.name}
                    %{~ endfor ~}
                  customMounts:
                    - name: sys-host
                      mountPath: /mnt/jail.upper/sys-host
                      readOnly: true
                      volumeSourceName: sys-host

              exporter:
                enabled: ${slurm_cluster.nodes.exporter.enabled}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.system.name}
                exporterContainer:
                  resources:
                    cpu: ${slurm_cluster.nodes.exporter.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.exporter.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.exporter.resources.ephemeral_storage}Gi

              rest:
                enabled: ${slurm_cluster.nodes.rest.enabled}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.system.name}
                size: 2
                rest:
                  resources:
                    cpu: ${slurm_cluster.nodes.rest.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.rest.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.rest.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]

            sConfigController:
              node:
                k8sNodeFilterName: ${slurm_cluster.sconfigcontroller.node.k8s_node_filter_name }
                size: ${slurm_cluster.sconfigcontroller.node.size }
              container:
                imagePullPolicy: ${slurm_cluster.sconfigcontroller.container.image_pull_policy }
                resources:
                  cpu: ${slurm_cluster.sconfigcontroller.container.resources.cpu }m
                  memory: ${slurm_cluster.sconfigcontroller.container.resources.memory }Mi
                  ephemeralStorage: ${slurm_cluster.sconfigcontroller.container.resources.ephemeral_storage }Mi

          slurmClusterStorage:
            enabled: true
            interval: 5m
            timeout: 5m
            overrideValues:
              volume:
                controllerSpool:
                  size: ${slurm_cluster_storage.volume.controller_spool.size}
                  filestoreDeviceName: ${slurm_cluster_storage.volume.controller_spool.device}
                jail:
                  size: ${slurm_cluster_storage.volume.jail.size}
                  filestoreDeviceName: ${slurm_cluster_storage.volume.jail.device}
                %{~ if length(slurm_cluster_storage.volume.jail_submounts) > 0 ~}
                jailSubMounts:
                  %{~ for sub_mount in slurm_cluster_storage.volume.jail_submounts ~}
                  - name: ${sub_mount.name}
                    size: ${sub_mount.size}
                    filestoreDeviceName: ${sub_mount.device}
                  %{~ endfor ~}
                %{~ endif ~}
                accounting:
                  enabled: ${slurm_cluster_storage.volume.accounting.enabled}
                  %{~ if slurm_cluster_storage.volume.accounting.enabled ~}
                  size: ${slurm_cluster_storage.volume.accounting.size}
                  filestoreDeviceName: ${slurm_cluster_storage.volume.accounting.device}
                  %{~ endif ~}

              storage:
                accounting:
                  matchExpressions:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: In
                      values:
                        - ${slurm_cluster_storage.scheduling.accounting.match}
                  tolerations:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: Equal
                      value: ${slurm_cluster_storage.scheduling.label.accounting}
                      effect: NoSchedule

                controllerSpool:
                  matchExpressions:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: In
                      values:
                        - ${slurm_cluster_storage.scheduling.controller.match}
                  tolerations:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: Equal
                      value: ${slurm_cluster_storage.scheduling.label.controller}
                      effect: NoSchedule

                jail:
                  matchExpressions:
                    - key: slurm.nebius.ai/jail
                      operator: In
                      values:
                        - "true"
                  tolerations:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: Exists
                      effect: NoSchedule
                  %{~ if slurm_cluster_storage.scheduling.worker.gpu_present ~}
                    - key: ${slurm_cluster_storage.scheduling.label.gpu}
                      operator: Exists
                      effect: NoSchedule
                  %{~ endif ~}

        soperatorActiveChecks:
          enabled: true
          interval: 5m
          timeout: 120m
          version: ${operator_version}
          releaseName: soperator-activechecks
          namespace: soperator
          overrideValues:
            clusterName: ${name}
            ${soperator_active_checks_override_block}

        soperator:
          enabled: true
          interval: 5m
          timeout: 5m
          version: ${operator_version}
          values:
            manager:
              resources:
                requests:
                  memory: ${resources.slurm_operator.requests.memory}Gi
                  cpu: ${resources.slurm_operator.requests.cpu * 1000}m
                limits:
                  memory: ${resources.slurm_operator.limits.memory}Gi
          kruise:
            enabled: true
            interval: 5m
            timeout: 5m
            overrideValues:
              manager:
                image:
                  repository: cr.eu-north1.nebius.cloud/soperator/kruise-manager
              helmHooks:
                image:
                  repository: cr.eu-north1.nebius.cloud/soperator/kruise-helm-hook
          soperatorChecks:
            enabled: true
            interval: 5m
            timeout: 5m
            values:
              checks:
                manager:
                  resources:
                    requests:
                      memory: ${resources.slurm_operator.requests.memory}Gi
                      cpu: ${resources.slurm_operator.requests.cpu * 1000}m
                    limits:
                      memory: ${resources.slurm_operator.limits.memory}Gi
          nodeConfigurator:
            enabled: true
            interval: 5m
            timeout: 5m
            values:
              resources:
                requests:
                  memory: ${resources.node_configurator.requests.memory}Gi
                  cpu: ${resources.node_configurator.requests.cpu * 1000}m
                limits:
                  memory: ${resources.node_configurator.limits.memory}Gi

        notifier:
          enabled: ${notifier.enabled}
          %{~ if notifier.enabled ~}
          values:
            slack:
              webhookUrl: "${notifier.slack_webhook_url}"
          %{~ endif ~}

        nfsServer:
          enabled: ${slurm_cluster.nfs_in_k8s.enabled}
          %{~ if slurm_cluster.nfs_in_k8s.enabled ~}
          version: ${operator_version}
          overrideValues:
            image:
              repository: ${soperator_image_repo}/nfs-server
              tag: ${operator_version}
            nameOverride: nfs-server
            nfs:
              sharedDirectory: "/exported"
              shareOptions: "rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash"
            storage:
              storageClassName: "${slurm_cluster.nfs_in_k8s.storage_class}"
              size: ${slurm_cluster.nfs_in_k8s.size_gibibytes}Gi
            storageClass:
              name: nfs
            resources:
              requests:
                memory: ${resources.nfs_server.requests.memory}Gi
                cpu: ${resources.nfs_server.requests.cpu * 1000}m
              limits:
                memory: ${resources.nfs_server.limits.memory}Gi
            %{~ if telemetry_enabled ~}
            monitoring:
              enabled: true
              serviceMonitor:
                enabled: true
            %{~ endif ~}
          %{~ endif ~}
        tailscale:
          enabled: ${tailscale_enabled}
