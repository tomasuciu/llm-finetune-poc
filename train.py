import torch
import torch.distributed as dist
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP
)
import transformers


def main():
    pass


if __name__ == "__main__":
    pass
